{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "th7lRKW98IJ4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "awaw_ND_1h2d"
      },
      "outputs": [],
      "source": [
        "!pip -q install pandas numpy scikit-learn tensorflow matplotlib joblib streamlit"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "up = files.upload()"
      ],
      "metadata": {
        "id": "gHz1wpCZ7NEx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_PATH = 'Crop_recommendation.csv'\n",
        "\n",
        "ARTIFACTS_DIR = Path(\"/content/artifacts\")\n",
        "ARTIFACTS_DIR.mkdir(exist_ok=True, parents=True)"
      ],
      "metadata": {
        "id": "ExAUMjVg8Ehv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q scikit-learn joblib gradio==4.44.0\n",
        "\n",
        "import os, json, joblib, numpy as np, pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, callbacks, utils\n",
        "tf.__version__"
      ],
      "metadata": {
        "id": "sY3R3htD8YSa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(DATA_PATH).drop_duplicates().dropna()\n",
        "\n",
        "FEATURES = [\"N\", \"P\", \"K\", \"temperature\", \"humidity\", \"ph\", \"rainfall\"]\n",
        "TARGET = \"label\"\n",
        "\n",
        "X = df[FEATURES].values.astype(\"float32\")\n",
        "y_text = df[TARGET].values"
      ],
      "metadata": {
        "id": "qOdCGhsF8p4a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y_text)\n",
        "num_classes = len(le.classes_)\n",
        "y_cat = utils.to_categorical(y, num_classes=num_classes)"
      ],
      "metadata": {
        "id": "hhFJo8lE9KNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "    X, y_cat, test_size=0.30, random_state=42, stratify=y_cat\n",
        ")\n",
        "X_val, X_test, y_val, y_test = train_test_split(\n",
        "    X_temp, y_temp, test_size=0.50, random_state=42, stratify=y_temp\n",
        ")"
      ],
      "metadata": {
        "id": "vH4WJbfv9TSu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "X_train_s = scaler.fit_transform(X_train)\n",
        "X_val_s   = scaler.transform(X_val)\n",
        "X_test_s  = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "tOM1aOq79Vrd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_int = np.argmax(y_train, axis=1)\n",
        "class_weights = compute_class_weight(\n",
        "    class_weight=\"balanced\",\n",
        "    classes=np.arange(num_classes),\n",
        "    y=y_train_int\n",
        ")\n",
        "class_weights = {i: w for i, w in enumerate(class_weights)}"
      ],
      "metadata": {
        "id": "cieY6Pwk9YdI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(input_dim, num_classes):\n",
        "    model = models.Sequential([\n",
        "        layers.Input(shape=(input_dim,)),\n",
        "        layers.Dense(128, activation=\"relu\"),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(0.25),\n",
        "\n",
        "        layers.Dense(64, activation=\"relu\"),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(0.25),\n",
        "\n",
        "        layers.Dense(32, activation=\"relu\"),\n",
        "        layers.BatchNormalization(),\n",
        "\n",
        "        layers.Dense(num_classes, activation=\"softmax\")\n",
        "    ])\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
        "        loss=\"categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"]\n",
        "    )\n",
        "    return model\n",
        "\n",
        "model = build_model(X_train_s.shape[1], num_classes)"
      ],
      "metadata": {
        "id": "9xXjo09y9bsN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "es = callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=15, restore_best_weights=True)\n",
        "rlr = callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=7, min_lr=1e-5)\n",
        "ckpt_path = ARTIFACTS_DIR / \"best_model.keras\"\n",
        "mc = callbacks.ModelCheckpoint(filepath=str(ckpt_path), monitor=\"val_accuracy\", save_best_only=True)\n",
        "\n",
        "# ---- Train\n",
        "history = model.fit(\n",
        "    X_train_s, y_train,\n",
        "    validation_data=(X_val_s, y_val),\n",
        "    epochs=200,\n",
        "    batch_size=32,\n",
        "    class_weight=class_weights,\n",
        "    callbacks=[es, rlr, mc],\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "id": "5J0NAFJR9geQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = tf.keras.models.load_model(ckpt_path)\n",
        "y_pred_probs = best_model.predict(X_test_s)\n",
        "y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_true, y_pred, target_names=le.classes_))\n",
        "print(f\"\\nTest Accuracy: {(y_true == y_pred).mean():.4f}\")\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_true, y_pred))"
      ],
      "metadata": {
        "id": "iMfj-mrn9l3e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "joblib.dump(scaler, ARTIFACTS_DIR / \"scaler.joblib\")\n",
        "with open(ARTIFACTS_DIR / \"label_encoder_classes.json\", \"w\") as f:\n",
        "    json.dump(le.classes_.tolist(), f)\n",
        "print(\"\\nSaved:\")\n",
        "print(f\"- Model: {ckpt_path}\")\n",
        "print(f\"- Scaler: {ARTIFACTS_DIR / 'scaler.joblib'}\")\n",
        "print(f\"- Label classes: {ARTIFACTS_DIR / 'label_encoder_classes.json'}\")"
      ],
      "metadata": {
        "id": "Zn8eMTTo9x4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np, json, joblib, tensorflow as tf\n",
        "\n",
        "best_model = tf.keras.models.load_model(\"/content/artifacts/best_model.keras\")\n",
        "scaler = joblib.load(\"/content/artifacts/scaler.joblib\")\n",
        "classes = json.loads(open(\"/content/artifacts/label_encoder_classes.json\").read())\n",
        "\n",
        "FEATURES = [\"N\",\"P\",\"K\",\"temperature\",\"humidity\",\"ph\",\"rainfall\"]\n",
        "\n",
        "sample = {\n",
        "    \"N\":90, \"P\":42, \"K\":43,\n",
        "    \"temperature\":26.5, \"humidity\":80.0, \"ph\":6.5, \"rainfall\":200.0\n",
        "}\n",
        "x = np.array([[sample[f] for f in FEATURES]], dtype=\"float32\")\n",
        "x_s = scaler.transform(x)\n",
        "probs = best_model.predict(x_s)[0]\n",
        "pred = classes[int(np.argmax(probs))]\n",
        "pred, float(np.max(probs))\n"
      ],
      "metadata": {
        "id": "a1visX9t92tF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Co8WsPa1-P7h"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}